{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this project, we develop a Deep Deterministic Policy Gradient (DDPG) agent that utilises its newly acquired skills to control a robotic arm, and steer it to a target location. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "### 0. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set environment parameters\n",
    "\n",
    "Here we set env_name to the name of the Unity environment file we want to launch. We should ensure that the environment build is in the python directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"./python/Reacher_Linux/Reacher.x86_64\"  # Name of the Unity environment binary to launch\n",
    "train_mode = True                                   # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.3 | packaged by conda-forge | (default, Nov  4 2017, 10:10:56) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "\n",
    "UnityEnvironment launches and begins communication with the environment when instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the State and Action Spaces\n",
    "\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Take Random Actions in the Environment\n",
    "\n",
    "Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions.\n",
    "\n",
    "Once this cell is executed, a message will be printed that detail how much reward was accumulated during one Episode. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the agent\n",
    "\n",
    "Now let us train our agent to solve the environment!\n",
    "\n",
    "#### 6.1 Model Definition\n",
    "\n",
    "To implement the [DDPG Algorithm](https://arxiv.org/abs/1509.02971), we define two simple neural networks, one for the Actor and one for the Critic. The networks have a few fully connected layers, with batch normalization in between. The latter helped stabilize the performance of the agent. For the Actor network we use ReLU activation functions on every layer except the last one, where we apply the tanh function, because we need to output values between -1 and 1. On the Critic network we again use mostly ReLU activations, except the last layer, that has no activation as it simply evaluates the output of the Actor.\n",
    "\n",
    "First, we need to import the necessary libraries and set the hyperparameters values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # tau parameter for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Actor model\n",
    "\n",
    "In the [DDPG paper](https://arxiv.org/abs/1509.02971), they introduced this algorithm as an \"Actor-Critic\" method. Though, some researchers think DDPG is best classified as a DQN method for continuous action spaces (along with [NAF](https://arxiv.org/abs/1603.00748)).\n",
    "\n",
    "In DDPG, every time we query the Actor network we want to get back the best believed action for a specific, given state. That is a deterministic policy. The Actor is basically learning the function: \n",
    "\n",
    "$$argmax_aQ(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        super(Actor, self).__init__() \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        # Batch normalization operations between layers\n",
    "        self.batch_norm1 = nn.BatchNorm1d(state_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(fc1_units)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(fc2_units)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Layer weight and bias initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc1.bias.data.fill_(.1)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc2.bias.data.fill_(.1)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        self.fc3.bias.data.fill_(.1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\" Build an actor (policy) network that maps states -> actions. \"\"\"\n",
    "        x = self.batch_norm1(state)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.tanh(self.fc3(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Critic model\n",
    "\n",
    "The Critic learns to evaluate the output of the Actor, which is the estimated best action to take, given a specific state. That is to evaluate the optimal action value function, by using the Actor's best believed action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):      \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        # Batch normalization operation\n",
    "        self.batch_norm = nn.BatchNorm1d(fc1_units)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Layer weight and bias initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc1.bias.data.fill_(.1)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc2.bias.data.fill_(.1)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        self.fc3.bias.data.fill_(.1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = self.relu(self.fc1(state))\n",
    "        x = self.batch_norm(x)\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Create the DDPG Agent\n",
    "\n",
    "First, we create the noise function, which helps with exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the experience replay buffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\" Initialize a ReplayBuffer object Parameters\n",
    "        \n",
    "        ----------\n",
    "        buffer_size (int): maximum size of buffer\n",
    "        batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size) # internal memory i.e. the buffer \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Add a new experience to the buffer. \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\" Randomly sample a batch from memory. \"\"\"        \n",
    "        batch = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in batch if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in batch if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in batch if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in batch if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in batch if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)                                    \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Return the current size of memory. \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we implement the DDPG agent that solves the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.hard_copy(self.critic_local, self.critic_target)\n",
    "        self.hard_copy(self.actor_local, self.actor_target)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_target_next = self.critic_target(next_states, actions_next)\n",
    "        # compute Q targets for next states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_target_next * (1.0 - dones))\n",
    "        # Compute citic loss \n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimise loss \n",
    "        self.critic_optimizer.zero_grad() \n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step() \n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def hard_copy(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ddpg` function trains the agent in the given environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    \n",
    "    for e in range(1, n_episodes+1):\n",
    "        # Reset environment \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # Reset agent \n",
    "        agent.reset() \n",
    "        # Get the initial state \n",
    "        state = env_info.vector_observations[0]      \n",
    "        # Reassign score to 0 \n",
    "        score = 0 \n",
    "        \n",
    "        for t in range(max_t):\n",
    "            # Get action \n",
    "            action = agent.act(state)           \n",
    "            # Observe reaction (environment)\n",
    "            env_info = env.step(action)[brain_name]        \n",
    "            ## Get new state \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            ## Get reward \n",
    "            reward = env_info.rewards[0]\n",
    "            # See if episode has finished \n",
    "            done = env_info.local_done[0]  \n",
    "            # Step \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            max_score = max(max_score, score)\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax score: {:.3f}'.format(e, np.mean(scores_deque), \n",
    "                                                                              max_score), end=\"\")\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e-100, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_checkpoint.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_checkpoint.pth')\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the DDPG method to train our agent. To solve the environment, the agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.23\tMax score: 4.560\n",
      "Episode 200\tAverage Score: 9.63\tMax score: 25.510\n",
      "Episode 300\tAverage Score: 28.65\tMax score: 39.430\n",
      "Episode 310\tAverage Score: 30.20\tMax score: 39.430\n",
      "Environment solved in 210 episodes!\tAverage Score: 30.20\n"
     ]
    }
   ],
   "source": [
    "# Initialize a DDPG Agent\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "\n",
    "# Run the algorithm\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Watch a trained agent\n",
    "\n",
    "Watch the performance of a trained agent in this environment. The agent's robotic arm succesfully follows the target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 29.759999334812164\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('critic_checkpoint.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state)                      # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Close the environment when finished\n",
    "\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Results\n",
    "\n",
    "Here we plot the evolution of the score accumulated by our agent, by the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5693d69e10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title(\"Score by Episode\")\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
